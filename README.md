# **음성 감정 인식 모델 성능 비교 보고서**

## **1. 서론**

음성 데이터를 기반으로 감정을 인식하는 것은 감정 컴퓨팅의 중요한 연구 주제 중 하나입니다.  
운전자와 같은 특정 상황에서 음성 데이터의 감정을 파악함으로써 스트레스 여부를 탐지하는 것은 운전 중 위험 상황을 예방하는 데 중요한 역할을 할 수 있습니다.  
본 연구에서는 **CNN**, **RNN**, **Transformer** 세 가지 모델을 활용하여 음성 데이터에서 감정을 분류하고 그 성능을 비교했습니다.  
각 모델은 **정확도(Accuracy)**, **정밀도(Precision)**, **재현율(Recall)**, **F1 Score**로 성능을 평가하였으며, 결과를 시각적으로 분석하기 위해 **혼동 행렬**과 **손실 그래프**를 사용했습니다.

<br>

## **2. 실험 설정**

### **2.1 데이터셋**

실험에 사용된 데이터셋은 **TESS (Toronto Emotional Speech Set)**로, 7가지 감정(angry, disgust, fear, happy, neutral, ps, sad)이 포함되어 있습니다.  
해당 감정들을 이진 분류 문제로 변환하여 **스트레스 상태(Stressed)**와 **비스트레스 상태(NotStressed)**로 나누었습니다.  
- **Stressed**: angry, disgust, fear, sad  
- **NotStressed**: happy, neutral, ps

음성 데이터는 **MFCC(Mel-Frequency Cepstral Coefficients)** 방식으로 음향 특징을 추출하여 모델의 입력으로 사용했습니다.

<br>

### **2.2 모델 아키텍처**

1. **CNN (Convolutional Neural Network)**  
   CNN은 음성 데이터의 2D 특성을 활용하여 감정을 분류했습니다.  
   3개의 **Conv2D** 레이어와 **Dropout**을 통해 과적합을 방지했으며, **ReLU** 활성화 함수로 비선형성을 추가했습니다.

2. **RNN (Recurrent Neural Network)**  
   RNN은 **LSTM (Long Short-Term Memory)** 레이어를 통해 음성 데이터의 시계열적 특성을 학습했습니다.  
   **4개의 LSTM 레이어**와 **Adaptive Average Pooling**을 적용해 감정 분류를 진행했습니다.

3. **Transformer**  
   트랜스포머 모델은 음성 데이터에서 중요한 패턴을 학습하기 위해 **4개의 Transformer Encoder 레이어**를 사용했습니다.  
   이 모델은 **Self-Attention** 메커니즘을 통해 음성 데이터의 전역적인 의존성을 학습하려고 시도했습니다.

<br>

### **2.3 학습 설정**

- **손실 함수**: **BCEWithLogitsLoss**를 사용하여 이진 분류 문제를 해결했습니다.
- **최적화 알고리즘**: **Adam Optimizer**를 사용하여 모델의 파라미터를 학습시켰습니다.
- **Epoch 수**: 각 모델은 **100 Epoch** 동안 학습을 진행했습니다.
- **배치 크기**: 배치 크기는 128로 설정되었습니다.
- **평가 지표**: Accuracy, Precision, Recall, F1 Score를 사용했습니다.

<br>

## **3. 결과 분석**

### **3.1 CNN 모델**

- **Accuracy**: 0.786
- **Precision**: [0.702, 0.994]
- **Recall**: [0.997, 0.577]
- **F1 Score**: [0.824, 0.730]

![CNN Model](attachment-link)

**CNN 모델**은 학습과 검증 손실이 비교적 안정적으로 감소했으나, **검증 손실의 급격한 변화**가 종종 발생했습니다.  
이는 **과적합**이 발생했을 가능성을 시사합니다.  
**Accuracy는 78.6%**로 나타났으며, 혼동 행렬에서 **Stressed** 데이터를 과소 분류하는 경향이 보였습니다.  
즉, CNN 모델은 **NotStressed** 데이터를 보다 정확하게 분류했으나 **Stressed** 데이터를 분류하는 데 어려움을 겪었습니다.

---

### **3.2 RNN 모델**

- **Accuracy**: 0.991
- **Precision**: [0.997, 0.987]
- **Recall**: [0.987, 0.997]
- **F1 Score**: [0.992, 0.992]

![RNN Model](attachment-link)

**RNN 모델**은 이번 실험에서 **가장 높은 성능**을 기록한 모델입니다.  
학습과 검증 손실 모두 안정적으로 감소하며, **Accuracy는 99.1%**에 도달했습니다.  
혼동 행렬에서도 거의 완벽한 분류 성능을 보여 **Stressed**와 **NotStressed** 데이터 모두에서 높은 성능을 기록했습니다.  
**LSTM** 구조 덕분에 음성 데이터의 시계열적 특성을 가장 잘 반영한 결과를 보여주었습니다.

---

### **3.3 Transformer 모델**

- **Accuracy**: 0.590
- **Precision**: [0.966, 0.550]
- **Recall**: [0.187, 0.993]
- **F1 Score**: [0.313, 0.708]

![Transformer Model](attachment-link)

**Transformer 모델**은 상대적으로 **낮은 성능**을 보였습니다.  
학습 손실은 꾸준히 감소했지만, 검증 손실이 안정적으로 감소하지 않았으며, **Accuracy는 59%**에 그쳤습니다.  
혼동 행렬에서 **Stressed** 데이터를 **NotStressed**로 오분류하는 빈도가 매우 높았으며, **NotStressed** 데이터를 분류하는 데는 비교적 우수한 성능을 보였습니다.  
이는 트랜스포머가 **시계열 데이터를 학습하는 데 부적합할 수 있다**는 결론을 시사합니다.

---

## **4. 결론**

이번 실험을 통해 **RNN 모델**이 음성 데이터 기반 감정 인식에서 **가장 높은 성능**을 기록한 모델임을 확인할 수 있었습니다.  
**LSTM** 구조는 음성 데이터의 **시계열적 특성**을 잘 반영할 수 있었으며, **Accuracy, Precision, Recall, F1 Score** 모든 지표에서 **우수한 성능**을 보여주었습니다.

- **CNN 모델**은 비교적 좋은 성능을 보였으나 **과적합**의 가능성이 있었습니다. **NotStressed** 데이터를 분류하는 데 강점을 보였습니다.
- **Transformer 모델**은 **시계열 데이터**를 학습하는 데 적합하지 않은 것으로 나타났으며, 상대적으로 낮은 정확도와 F1 Score를 기록했습니다.

<br>

### **향후 연구 방향**

- **더 복잡한 RNN 구조**(예: **GRU** 또는 **Bi-LSTM**)를 도입하여 성능을 더욱 개선할 수 있습니다.
- **데이터 증강 기법**을 사용해 더 다양한 상황에서의 음성 데이터를 확보하여, 모델의 일반화 성능을 높일 수 있습니다.

<br>

### **참고 문헌**

- Toronto Emotional Speech Set (TESS)
- 관련된 음성 감정 인식 및 시계열 모델 연구

---

### **최종 결론**
이번 연구에서는 **CNN**, **RNN**, **Transformer** 모델을 사용하여 음성 감정 인식을 수행하였습니다. 그 결과, **RNN** 모델이 **시계열적 특성을 가장 잘 반영**하여 **가장 높은 성능**을 보였으며, **CNN**은 준수한 성능을 보였으나 **과적합** 가능성이 있었습니다. **Transformer** 모델은 성능이 저조했으며, **시계열 데이터 학습에 적합하지 않다**는 결론을 내릴 수 있었습니다.
